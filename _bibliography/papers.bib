---
---

@ARTICLE{INFER,
   author = {Srikanth, Shashank and Ansari, {Junaid Ahmed} and {Karnik Ram}, R. and Sharma, Sarthak and J. {Krishna Murthy} and {Madhava Krishna}, K.},
    title = "{INFER:  INtermediate  representations  for  FuturE  pRediction}",
  journal = {arXiv},
     year = 2019,
    month = mar,
    arxiv = {1903.10641},
    projectpage = {https://talsperre.github.io/INFER/},
    video = {https://www.youtube.com/watch?v=sHxXIX-FZoU&feature=youtu.be},
    code={https://github.com/talsperre/INFER},
    image = {infer.gif},
    abstract = {Deep learning methods have ushered in a new era for computer vision and robotics. With very accurate methods for object detection and semantic segmentation, we are now at a juncture where we can envisage the application of these techniques to perform higher-order understanding. One such application which we consider in this work, is predicting future states of traffic participants in urban driving scenarios. Specifically, we argue that constructing intermediate representations of the world using off-the-shelf computer vision models for semantic segmentation and object detection, we can train models that account for the multi-modality of future states, and at the same time transfer well across different train and test distributions (datasets). Our approach, dubbed INFER (INtermediate representations for distant FuturE pRediction), involves training an autoregressive model that takes in an intermediate representation of past states of the world, and predicts a multimodal distribution over plausible future states. The model consists of an Encoder-Decoder with ConvLSTM present along the skip connections, and in between the Encoder-Decoder. The network takes an intermediate representation of the scene and predicts the future locations of the Vehicle of Interest (VoI). We outperform the current best future prediction model on KITTI while predicting deep into the future (3 sec, 4 sec) by a significant margin. Contrary to most approaches dealing with future prediction that do not generalize well to datasets that they have not been trained on, we test our method on different datasets like Oxford RobotCar and Cityscapes, and show that the network performs well across these datasets which differ in scene layout, weather conditions, and also generalizes well across cross-sensor modalities. We carry out a thorough ablation study on our intermediate representation that captures the role played by different semantics. We conclude the results section by showcasing an important use case of future prediction : multi object tracking and exhibit results on select sequences from KITTI and Cityscapes.},
}

@ARTICLE{DAL,
   author = {Krishna, Sai and Seo, Keehong and Bhatt, Dhaivat and Mai, Vincent and J. {Krishna Murthy} and Paull, Liam},
    title = "{Deep Active Localization}",
  journal = {arXiv},
     year = 2019,
    month = feb,
    arxiv = {1903.01669},
    projectpage = {https://github.com/montrealrobotics/dal},
    image = {dal.png},
    abstract = {Active localization is the problem of generating robot actions that allow it to maximally disambiguate its pose within a reference map. Traditional approaches to this use an information-theoretic criterion for action selection and hand-crafted perceptual models. In this work we propose an end-to-end differentiable method for learning to take informative actions that is trainable entirely in simulation and then transferable to real robot hardware with zero refinement. The system is composed of two modules: a convolutional neural network for perception, and a deep reinforcement learned planning module. We introduce a multi-scale approach to the learned perceptual model since the accuracy needed to perform action selection with reinforcement learning is much less than the accuracy needed for robot control. We demonstrate that the resulting system outperforms using the traditional approach for either perception or planning. We also demonstrate our approaches robustness to different map configurations and other nuisance parameters through the use of domain randomization in training. The code is also compatible with the OpenAI gym framework, as well as the Gazebo simulator.},
}

@ARTICLE{CTCNet,
   author = {Iyer, Ganesh, and J. {Krishna Murthy} and Gupta, Gunshi and {Madhava Krishna}, K. and Paull, Liam},
    title = "{Geometric Consistency for Self-Supervised End-to-End Visual Odometry}",
  journal = {1st International Workshop on Deep Learning for Visual SLAM, CVPR},
     year = 2018,
    month = apr,
    arxiv = {1804.03789},
    projectpage = {https://krrish94.github.io/CTCNet-release/},
    image = {ctcnet.png},
    abstract = {With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.},
}

@ARTICLE{slope,
   author = {Ansari, {Junaid Ahmed} and Sharma, Sarthak and Majumdar, Anshuman and J. {Krishna Murthy} and {Madhava Krishna}, K.
  },
    title = "{The Earth ain't Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera}",
  journal = {IROS},
     year = 2018,
    month = mar,
    arxiv = {1803.02057},
    video = {https://www.youtube.com/watch?v=KQZFa5_IvpU},
    image = {slope.png},
    abstract = {Accurate localization of other traffic participants is a vital task in autonomous driving systems. State-of-the-art systems employ a combination of sensing modalities such as RGB cameras and LiDARs for localizing traffic participants, but most such demonstrations have been confined to plain roads. We demonstrate, to the best of our knowledge, the first results for monocular object localization and shape estimation on surfaces that do not share the same plane with the moving monocular camera. We approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the pose and shape of the vehicles, and the orientation of the local ground plane on which the vehicle stands as well. We evaluate the proposed approach on the KITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations. The proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads.}
}

@ARTICLE{CalibNet,
   author = {{Iyer}, Ganesh, and {Ram}, Karnik, and J. {Krishna Murthy} and {Madhava Krishna}, K.
  },
    title = "{CalibNet: Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks}",
  journal = {IROS},
     year = {2018},
    month = mar,
    arxiv = {1803.08181},
    projectpage = {https://epiception.github.io/CalibNet/},
    video = {https://youtu.be/WyW9T2dSbec},
    image = {calibnet.png},
    abstract = {3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation.},
}

% IROS 2017
@ARTICLE{Krishna_IROS2017,
  title={Shape Priors for Real-Time Monocular Object Localization in Dynamic Environments},
  author={J. {Krishna Murthy} and Sharma, Sarthak and K. {Madhava Krishna}},
  journal = {IROS},
  year={2017},
  month=oct,
  pdf={2017iros.pdf},
  poster={2017iros_poster.pdf},
  video={https://youtu.be/Of-P34rWBBw},
  image = {iros2017.png},
  abstract = {Reconstruction of dynamic objects in a scene is a highly challenging problem in the context of SLAM. In this paper, we present a real-time monocular object localization system that estimates the shape and pose of dynamic objects in real-time, using video frames captured from a moving monocular camera. Although the problem seems to be ill-posed, we demonstrate that, by incorporating prior knowledge of the object category, we can obtain more detailed instance-level reconstructions. As opposed to earlier object model specifications, the proposed shape-prior model leads to the formulation of a Bundle Adjustment-like optimization problem for simultaneous shape and pose estimation. Leveraging recent successes of Convolutional Neural Networks (CNNs) for object keypoint localization, we present a CNN architecture that performs precise keypoint localization. We then demonstrate how these keypoints can be used to recover 3D object properties, while accounting for any 2D localization errors and self-occlusion. We show significant performance improvements compared to state-of-the-art monocular competitors for 2D keypoint detection, as well as 3D localization and reconstruction of dynamic objects.},
}


% ICRA 2017
@ARTICLE{Krishna_ICRA2017,
  title={Reconstructing Vehicles From a Single Image: Shape Priors for Road Scene Understanding},
  author={J. {Krishna Murthy} and G.V. {Sai Krishna} and Chhaya, Falak and K. {Madhava Krishna}},
  journal = {ICRA},
  year={2017},
  month=june,
  pdf={2017icra.pdf},
  poster={2017icra_poster.pdf},
  video={https://youtu.be/rSMjHZV4axg},
  image = {icra2017.png},
  code = {https://github.com/krrish94/ICRA2017},
  abstract = {We present an approach for reconstructing vehicles from a single (RGB) image, in the context of autonomous driving. Though the problem appears to be ill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles project to an image can be used to reason about the reverse process, i.e., how shapes (back-)project from 2D to 3D. We encode this knowledge in shape priors, which are learnt over a small keypoint-annotated dataset. We then formulate a shape-aware adjustment problem that uses the learnt shape priors to recover the 3D pose and shape of a query object from an image. For shape representation and inference, we leverage recent successes of Convolutional Neural Networks (CNNs) for the task of object and keypoint localization, and train a novel cascaded fully-convolutional architecture to localize vehicle keypoints in images. The shape-aware adjustment then robustly recovers shape (3D locations of the detected keypoints) while simultaneously filling in occluded keypoints. To tackle estimation errors incurred due to erroneously detected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS) scheme for robust optimization, and as a by-product characterize noise models for each predicted keypoint. We evaluate our approach on autonomous driving benchmarks, and present superior results to existing monocular, as well as stereo approaches.},
}

@ARTICLE{Parv_ICRA2018,
  title={Constructing Category-Specific Models for Monocular Object SLAM},
  author={Parv Parkhiya and Rishabh Khawad and J. {Krishna Murthy} and {Madhava Krishna}, K. and Brojeshwar Bhowmick},
  journal = {ICRA},
  year={2018},
  arxiv={1802.09292},
  video={https://youtu.be/_LpHrn1opSk},
  image = {objectslam.png},
  abstract = {We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fly, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show --- to the best of our knowledge --- first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.},
}

@article{Junaid_ICRA2018,
  title={Beyond Pixels: Leveraging Geometry and Shape Cues for Multi-Object Tracking},
  author={Sarthak Sharma and Junaid Ahmed Ansari and J. {Krishna Murthy} and K. Madhava Krishna},
  journal = {ICRA},
  year={2018},
  arxiv={1802.09298},
  code = {https://github.com/JunaidCS032/MOTBeyondPixels},
  video={https://youtu.be/SfoK8u2_s-o},
  image = {tracking.png},
  abstract = {This paper introduces geometry and object shape and pose costs for multi-object tracking in urban driving scenarios. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes.},
}

@INPROCEEDINGS{Maxxyt, 
author={D. Pruthi and A. Jain and J. {Krishna Murthy} and R. Nalwaya and P. Teja}, 
booktitle={2015 17th UKSim-AMSS International Conference on Modelling and Simulation (UKSim)}, 
title={Maxxyt: An Autonomous Wearable Device for Real-Time Tracking of a Wide Range of Exercises}, 
year={2015}, 
}

@article{FAST,
  title={FAST},
  author={Gautam, Avinash and Jha, Bhargav and Kumar, Gourav and J. {Krishna Murthy} and Ram, SP Arjun and Mohan, Sudeept},
  journal={Journal of Intelligent \& Robotic Systems},
  year={2017},
  publisher={Springer}
}

@INPROCEEDINGS{CAC, 
author={A. Gautam and J. {Krishna Murthy} and G. Kumar and S. P. A. Ram and B. Jha and S. Mohan}, 
booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics}, 
title={Cluster, Allocate, Cover: An Efficient Approach for Multi-robot Coverage}, 
year={2015}}

