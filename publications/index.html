<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Krishna Murthy | publications</title>
  <meta name="description" content="Krishna's homepage. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="/assets/img/favicon.png">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Krishna</strong> Murthy
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- CV link -->
        <a class="page-link" href="/assets/pdf/CV.pdf">CV</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
            <a class="page-link" href="/teaching/">teaching</a>
          
        
          
        

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">Publications (reverse chronological order)</h5>
  </header>

  <article class="post-content publications clearfix">
    
<h3 class="year">2019</h3>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="INFER">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/infer.gif" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>INFER:  INtermediate  representations  for  FuturE  pRediction</b></span>
    <span class="author">
      
      	
        
          
            
              <a href="https://talsperre.github.io/" target="_blank">Shashank Srikanth</a>,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=Uc8mKqMAAAAJ&amp;hl=en" target="_blank">Junaid Ahmed Ansari</a>,
            
          
        
      
      	
        
          
            
              R. Karnik Ram,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=4uKV9aIAAAAJ&amp;hl=en" target="_blank">Sarthak Sharma</a>,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          and
          
            
              <a href="https://robotics.iiit.ac.in/" target="_blank">K. Madhava Krishna</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1903.10641" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
    <a href="https://talsperre.github.io/INFER/" target="_blank" class="buttonPP">Project Page</a>
  
  
    <a href="https://github.com/talsperre/INFER" target="_blank" class="buttonPP">Code</a>
  
  
    <a href="https://www.youtube.com/watch?v=sHxXIX-FZoU&amp;feature=youtu.be" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Deep learning methods have ushered in a new era for computer vision and robotics. With very accurate methods for object detection and semantic segmentation, we are now at a juncture where we can envisage the application of these techniques to perform higher-order understanding. One such application which we consider in this work, is predicting future states of traffic participants in urban driving scenarios. Specifically, we argue that constructing intermediate representations of the world using off-the-shelf computer vision models for semantic segmentation and object detection, we can train models that account for the multi-modality of future states, and at the same time transfer well across different train and test distributions (datasets). Our approach, dubbed INFER (INtermediate representations for distant FuturE pRediction), involves training an autoregressive model that takes in an intermediate representation of past states of the world, and predicts a multimodal distribution over plausible future states. The model consists of an Encoder-Decoder with ConvLSTM present along the skip connections, and in between the Encoder-Decoder. The network takes an intermediate representation of the scene and predicts the future locations of the Vehicle of Interest (VoI). We outperform the current best future prediction model on KITTI while predicting deep into the future (3 sec, 4 sec) by a significant margin. Contrary to most approaches dealing with future prediction that do not generalize well to datasets that they have not been trained on, we test our method on different datasets like Oxford RobotCar and Cityscapes, and show that the network performs well across these datasets which differ in scene layout, weather conditions, and also generalizes well across cross-sensor modalities. We carry out a thorough ablation study on our intermediate representation that captures the role played by different semantics. We conclude the results section by showcasing an important use case of future prediction : multi object tracking and exhibit results on select sequences from KITTI and Cityscapes.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ INFER, <br />
      author = {  
                  
                      Srikanth, Shashank and
                    
                   
                  
                      Ansari, Junaid Ahmed and
                    
                   
                  
                      Karnik Ram, R. and
                    
                   
                  
                      Sharma, Sarthak and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Madhava Krishna, K. 
                    
                   }, <br />
      title = { INFER:  INtermediate  representations  for  FuturE  pRediction }, <br />
      
        journal = { arXiv }, <br />
      
      
      
        year = { 2019 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="DAL">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/dal.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Deep Active Localization</b></span>
    <span class="author">
      
      	
        
          
            
              Sai Krishna,
            
          
        
      
      	
        
          
            
              Keehong Seo,
            
          
        
      
      	
        
          
            
              <a href="https://dhaivat666.github.io/" target="_blank">Dhaivat Bhatt</a>,
            
          
        
      
      	
        
          
            
              Vincent Mai,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          and
          
            
              <a href="http://liampaull.ca" target="_blank">Liam Paull</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1903.01669" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
    <a href="https://github.com/montrealrobotics/dal" target="_blank" class="buttonPP">Project Page</a>
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Active localization is the problem of generating robot actions that allow it to maximally disambiguate its pose within a reference map. Traditional approaches to this use an information-theoretic criterion for action selection and hand-crafted perceptual models. In this work we propose an end-to-end differentiable method for learning to take informative actions that is trainable entirely in simulation and then transferable to real robot hardware with zero refinement. The system is composed of two modules: a convolutional neural network for perception, and a deep reinforcement learned planning module. We introduce a multi-scale approach to the learned perceptual model since the accuracy needed to perform action selection with reinforcement learning is much less than the accuracy needed for robot control. We demonstrate that the resulting system outperforms using the traditional approach for either perception or planning. We also demonstrate our approaches robustness to different map configurations and other nuisance parameters through the use of domain randomization in training. The code is also compatible with the OpenAI gym framework, as well as the Gazebo simulator.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ DAL, <br />
      author = {  
                  
                      Krishna, Sai and
                    
                   
                  
                      Seo, Keehong and
                    
                   
                  
                      Bhatt, Dhaivat and
                    
                   
                  
                      Mai, Vincent and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Deep Active Localization }, <br />
      
        journal = { arXiv }, <br />
      
      
      
        year = { 2019 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="CTCNet">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/ctcnet.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Geometric Consistency for Self-Supervised End-to-End Visual Odometry</b></span>
    <span class="author">
      
      	
        
          
            
              <a href="https://epiception.github.io/" target="_blank">Ganesh Iyer</a>,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          
            
              <a href="https://gunshi.github.io/" target="_blank">Gunshi Gupta</a>,
            
          
        
      
      	
        
          
            
              <a href="https://robotics.iiit.ac.in/" target="_blank">K. Madhava Krishna</a>,
            
          
        
      
      	
        
          and
          
            
              <a href="http://liampaull.ca" target="_blank">Liam Paull</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>1st International Workshop on Deep Learning for Visual SLAM, CVPR</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1804.03789" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
    <a href="https://krrish94.github.io/CTCNet-release/" target="_blank" class="buttonPP">Project Page</a>
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ CTCNet, <br />
      author = {  
                  
                      Iyer, Ganesh and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Gupta, Gunshi and
                    
                   
                  
                      Madhava Krishna, K. and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Geometric Consistency for Self-Supervised End-to-End Visual Odometry }, <br />
      
        journal = { 1st International Workshop on Deep Learning for Visual SLAM, CVPR }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="slope">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/slope.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>The Earth ain’t Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera</b></span>
    <span class="author">
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=Uc8mKqMAAAAJ&amp;hl=en" target="_blank">Junaid Ahmed Ansari</a>,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=4uKV9aIAAAAJ&amp;hl=en" target="_blank">Sarthak Sharma</a>,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=IN-wDIMAAAAJ&amp;hl=en" target="_blank">Anshuman Majumdar</a>,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          and
          
            
              <a href="https://robotics.iiit.ac.in/" target="_blank">K. Madhava Krishna</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IROS</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1803.02057" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  
    <a href="https://www.youtube.com/watch?v=KQZFa5_IvpU" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Accurate localization of other traffic participants is a vital task in autonomous driving systems. State-of-the-art systems employ a combination of sensing modalities such as RGB cameras and LiDARs for localizing traffic participants, but most such demonstrations have been confined to plain roads. We demonstrate, to the best of our knowledge, the first results for monocular object localization and shape estimation on surfaces that do not share the same plane with the moving monocular camera. We approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the pose and shape of the vehicles, and the orientation of the local ground plane on which the vehicle stands as well. We evaluate the proposed approach on the KITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations. The proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ slope, <br />
      author = {  
                  
                      Ansari, Junaid Ahmed and
                    
                   
                  
                      Sharma, Sarthak and
                    
                   
                  
                      Majumdar, Anshuman and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Madhava Krishna, K. 
                    
                   }, <br />
      title = { The Earth ain’t Flat: Monocular Reconstruction of Vehicles on Steep and Graded Roads from a Moving Camera }, <br />
      
        journal = { IROS }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="CalibNet">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/calibnet.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>CalibNet: Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks</b></span>
    <span class="author">
      
      	
        
          
            
              <a href="https://epiception.github.io/" target="_blank">Ganesh Iyer</a>,
            
          
        
      
      	
        
          
            
              <a href="http://karnikram.info/" target="_blank">Karnik Ram</a>,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          and
          
            
              <a href="https://robotics.iiit.ac.in/" target="_blank">K. Madhava Krishna</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IROS</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1803.08181" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
    <a href="https://epiception.github.io/CalibNet/" target="_blank" class="buttonPP">Project Page</a>
  
  
  
    <a href="https://youtu.be/WyW9T2dSbec" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ CalibNet, <br />
      author = {  
                  
                      Iyer, Ganesh and
                    
                   
                  
                      Ram, Karnik and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Madhava Krishna, K. 
                    
                   }, <br />
      title = { CalibNet: Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks }, <br />
      
        journal = { IROS }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="Parv_ICRA2018">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/objectslam.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Constructing Category-Specific Models for Monocular Object SLAM</b></span>
    <span class="author">
      
      	
        
          
            
              <a href="https://parvparkhiya.github.io/" target="_blank">Parv Parkhiya</a>,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.co.in/citations?user=gaSCX-kAAAAJ&amp;hl=en" target="_blank">Rishabh Khawad</a>,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          
            
              <a href="https://robotics.iiit.ac.in/" target="_blank">K. Madhava Krishna</a>,
            
          
        
      
      	
        
          and
          
            
              <a href="https://sites.google.com/view/brojeshwar/home" target="_blank">Brojeshwar Bhowmick</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>ICRA</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1802.09292" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  
    <a href="https://youtu.be/_LpHrn1opSk" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-level models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fly, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show — to the best of our knowledge — first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ Parv_ICRA2018, <br />
      author = {  
                  
                      Parkhiya, Parv and
                    
                   
                  
                      Khawad, Rishabh and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Madhava Krishna, K. and
                    
                   
                  
                      Bhowmick, Brojeshwar 
                    
                   }, <br />
      title = { Constructing Category-Specific Models for Monocular Object SLAM }, <br />
      
        journal = { ICRA }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="Junaid_ICRA2018">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/tracking.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Beyond Pixels: Leveraging Geometry and Shape Cues for Multi-Object Tracking</b></span>
    <span class="author">
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=4uKV9aIAAAAJ&amp;hl=en" target="_blank">Sarthak Sharma</a>,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=Uc8mKqMAAAAJ&amp;hl=en" target="_blank">Junaid Ahmed Ansari</a>,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          and
          
            
              <a href="https://robotics.iiit.ac.in/" target="_blank">K. Madhava Krishna</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>ICRA</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1802.09298" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
    <a href="https://github.com/JunaidCS032/MOTBeyondPixels" target="_blank" class="buttonPP">Code</a>
  
  
    <a href="https://youtu.be/SfoK8u2_s-o" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>This paper introduces geometry and object shape and pose costs for multi-object tracking in urban driving scenarios. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ Junaid_ICRA2018, <br />
      author = {  
                  
                      Sharma, Sarthak and
                    
                   
                  
                      Ansari, Junaid Ahmed and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Krishna, K. Madhava 
                    
                   }, <br />
      title = { Beyond Pixels: Leveraging Geometry and Shape Cues for Multi-Object Tracking }, <br />
      
        journal = { ICRA }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<h3 class="year">2017</h3>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="Krishna_IROS2017">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/iros2017.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Shape Priors for Real-Time Monocular Object Localization in Dynamic Environments</b></span>
    <span class="author">
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=4uKV9aIAAAAJ&amp;hl=en" target="_blank">Sarthak Sharma</a>,
            
          
        
      
      	
        
          and
          
            
              <a href="https://robotics.iiit.ac.in/" target="_blank">K. Madhava Krishna</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IROS</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
  
  
    <a href="/assets/pdf/2017iros.pdf" target="_blank" class="buttonTT">PDF</a>
  
  
  
    <a href="/assets/pdf/2017iros_poster.pdf" target="_blank" class="buttonMM">Poster</a>
  
  
  
  
  
    <a href="https://youtu.be/Of-P34rWBBw" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Reconstruction of dynamic objects in a scene is a highly challenging problem in the context of SLAM. In this paper, we present a real-time monocular object localization system that estimates the shape and pose of dynamic objects in real-time, using video frames captured from a moving monocular camera. Although the problem seems to be ill-posed, we demonstrate that, by incorporating prior knowledge of the object category, we can obtain more detailed instance-level reconstructions. As opposed to earlier object model specifications, the proposed shape-prior model leads to the formulation of a Bundle Adjustment-like optimization problem for simultaneous shape and pose estimation. Leveraging recent successes of Convolutional Neural Networks (CNNs) for object keypoint localization, we present a CNN architecture that performs precise keypoint localization. We then demonstrate how these keypoints can be used to recover 3D object properties, while accounting for any 2D localization errors and self-occlusion. We show significant performance improvements compared to state-of-the-art monocular competitors for 2D keypoint detection, as well as 3D localization and reconstruction of dynamic objects.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ Krishna_IROS2017, <br />
      author = {  
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Sharma, Sarthak and
                    
                   
                  
                      Madhava Krishna, K. 
                    
                   }, <br />
      title = { Shape Priors for Real-Time Monocular Object Localization in Dynamic Environments }, <br />
      
        journal = { IROS }, <br />
      
      
      
        year = { 2017 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="Krishna_ICRA2017">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="/assets/img/icra2017.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Reconstructing Vehicles From a Single Image: Shape Priors for Road Scene Understanding</b></span>
    <span class="author">
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          
            
              <a href="https://saikrishna-1996.github.io/" target="_blank">G.V. Sai Krishna</a>,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.ca/citations?user=h3eZWsgAAAAJ&amp;hl=en" target="_blank">Falak Chhaya</a>,
            
          
        
      
      	
        
          and
          
            
              <a href="https://robotics.iiit.ac.in/" target="_blank">K. Madhava Krishna</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>ICRA</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
  
  
    <a href="/assets/pdf/2017icra.pdf" target="_blank" class="buttonTT">PDF</a>
  
  
  
    <a href="/assets/pdf/2017icra_poster.pdf" target="_blank" class="buttonMM">Poster</a>
  
  
  
  
    <a href="https://github.com/krrish94/ICRA2017" target="_blank" class="buttonPP">Code</a>
  
  
    <a href="https://youtu.be/rSMjHZV4axg" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>We present an approach for reconstructing vehicles from a single (RGB) image, in the context of autonomous driving. Though the problem appears to be ill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles project to an image can be used to reason about the reverse process, i.e., how shapes (back-)project from 2D to 3D. We encode this knowledge in shape priors, which are learnt over a small keypoint-annotated dataset. We then formulate a shape-aware adjustment problem that uses the learnt shape priors to recover the 3D pose and shape of a query object from an image. For shape representation and inference, we leverage recent successes of Convolutional Neural Networks (CNNs) for the task of object and keypoint localization, and train a novel cascaded fully-convolutional architecture to localize vehicle keypoints in images. The shape-aware adjustment then robustly recovers shape (3D locations of the detected keypoints) while simultaneously filling in occluded keypoints. To tackle estimation errors incurred due to erroneously detected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS) scheme for robust optimization, and as a by-product characterize noise models for each predicted keypoint. We evaluate our approach on autonomous driving benchmarks, and present superior results to existing monocular, as well as stereo approaches.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ Krishna_ICRA2017, <br />
      author = {  
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Sai Krishna, G.V. and
                    
                   
                  
                      Chhaya, Falak and
                    
                   
                  
                      Madhava Krishna, K. 
                    
                   }, <br />
      title = { Reconstructing Vehicles From a Single Image: Shape Priors for Road Scene Understanding }, <br />
      
        journal = { ICRA }, <br />
      
      
      
        year = { 2017 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="FAST">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>FAST</b></span>
    <span class="author">
      
      	
        
          
            
              <a href="https://universe.bits-pilani.ac.in/pilani/avinash/profile" target="_blank">Avinash Gautam</a>,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.co.il/citations?user=NpLE5zAAAAAJ&amp;hl=en" target="_blank">Bhargav Jha</a>,
            
          
        
      
      	
        
          
            
              <a href="https://scholar.google.co.in/citations?user=zyYNatcAAAAJ&amp;hl=en" target="_blank">Gourav Kumar</a>,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          
            
              <a href="https://scholar.google.com/citations?user=BmNZ0LAAAAAJ&amp;hl=en" target="_blank">SP Arjun Ram</a>,
            
          
        
      
      	
        
          and
          
            
              <a href="https://universe.bits-pilani.ac.in/pilani/sudeeptm/profile" target="_blank">Sudeept Mohan</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Intelligent &amp; Robotic Systems</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ FAST, <br />
      author = {  
                  
                      Gautam, Avinash and
                    
                   
                  
                      Jha, Bhargav and
                    
                   
                  
                      Kumar, Gourav and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Ram, SP Arjun and
                    
                   
                  
                      Mohan, Sudeept 
                    
                   }, <br />
      title = { FAST }, <br />
      
        journal = { Journal of Intelligent &amp; Robotic Systems }, <br />
      
      
      
        year = { 2017 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<h3 class="year">2015</h3>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="Maxxyt">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Maxxyt: An Autonomous Wearable Device for Real-Time Tracking of a Wide Range of Exercises</b></span>
    <span class="author">
      
      	
        
          
            
              D. Pruthi,
            
          
        
      
      	
        
          
            
              A. Jain,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          
            
              R. Nalwaya,
            
          
        
      
      	
        
          and
          
            
              P. Teja
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In 2015 17th UKSim-AMSS International Conference on Modelling and Simulation (UKSim)</em>
    
    
      2015
    
    </span>
  

  <span class="links">
  
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ Maxxyt, <br />
      author = {  
                  
                      Pruthi, D. and
                    
                   
                  
                      Jain, A. and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Nalwaya, R. and
                    
                   
                  
                      Teja, P. 
                    
                   }, <br />
      title = { Maxxyt: An Autonomous Wearable Device for Real-Time Tracking of a Wide Range of Exercises }, <br />
      
      
        journal = { 2015 17th UKSim-AMSS International Conference on Modelling and Simulation (UKSim) }, <br />
      
      
        year = { 2015 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="CAC">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Cluster, Allocate, Cover: An Efficient Approach for Multi-robot Coverage</b></span>
    <span class="author">
      
      	
        
          
            
              A. Gautam,
            
          
        
      
      	
        
          
            <em>J. Krishna Murthy</em>,
          
        
      
      	
        
          
            
              G. Kumar,
            
          
        
      
      	
        
          
            
              S. P. A. Ram,
            
          
        
      
      	
        
          
            
              B. Jha,
            
          
        
      
      	
        
          and
          
            
              S. Mohan
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In 2015 IEEE International Conference on Systems, Man, and Cybernetics</em>
    
    
      2015
    
    </span>
  

  <span class="links">
  
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ CAC, <br />
      author = {  
                  
                      Gautam, A. and
                    
                   
                  
                      Krishna Murthy, J. and
                    
                   
                  
                      Kumar, G. and
                    
                   
                  
                      Ram, S. P. A. and
                    
                   
                  
                      Jha, B. and
                    
                   
                  
                      Mohan, S. 
                    
                   }, <br />
      title = { Cluster, Allocate, Cover: An Efficient Approach for Multi-robot Coverage }, <br />
      
      
        journal = { 2015 IEEE International Conference on Systems, Man, and Cybernetics }, <br />
      
      
        year = { 2015 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>


  </article>

  
  <br>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2019 Krishna Murthy.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with a modified <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
        Last updated: December 02 2018.
    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-130243767-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
